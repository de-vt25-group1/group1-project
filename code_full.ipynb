{"cells":[{"cell_type":"code","execution_count":1,"id":"6154c5cd","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/03/10 20:31:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"reddit\").getOrCreate()"]},{"cell_type":"code","execution_count":2,"id":"d6623ec1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 1:======================================================>(146 + 1) / 147]\r"]},{"name":"stdout","output_type":"stream","text":["Current amount of rows: 3848330\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Load Reddit dataset which is up on Spark.\n","file_path = \"hdfs://de-project-g1-m:8020/data/reddit.json\" # For now use a smaller subset to simply test.\n","\n","df = spark.read.json(file_path)\n","rows_1 = df.count()\n","print(f\"Current amount of rows: {rows_1}\")"]},{"cell_type":"code","execution_count":3,"id":"045da64a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Current amount of rows: 3848330\n","+--------------------+-----------+\n","|                body|  subreddit|\n","+--------------------+-----------+\n","|I think it should...|       math|\n","|Art is about the ...|      funny|\n","|Ask me what I thi...|Borderlands|\n","|In Mechwarrior On...|   gamingpc|\n","|You are talking a...|     Diablo|\n","+--------------------+-----------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.sql.functions import col\n","# Select the only two we wanna analyze, the subreddit name and it's contents.\n","df = df.select(col(\"body\"), col(\"subreddit\"))\n","rows_2 = df.count()\n","print(f\"Current amount of rows: {rows_2}\")\n","df.show(5)"]},{"cell_type":"code","execution_count":4,"id":"8447db19","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 8:======================================================>(145 + 2) / 147]\r"]},{"name":"stdout","output_type":"stream","text":["Current amount of rows: 3848194, NULL rows dropped: 136.\n","+--------------------+--------------------+\n","|                body|           subreddit|\n","+--------------------+--------------------+\n","|I think it should...|                math|\n","|Art is about the ...|               funny|\n","|Ask me what I thi...|         Borderlands|\n","|In Mechwarrior On...|            gamingpc|\n","|You are talking a...|              Diablo|\n","|All but one of my...|   RedditLaqueristas|\n","|I could give a sh...|               apple|\n","|So you're saying ...|               apple|\n","|I love this idea ...|RedditFilmsProduc...|\n","|Theres an entire ...|       AbandonedPorn|\n","+--------------------+--------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Cleanup the data by removing null rows as we can see some above.\n","df = df.dropna(subset=[\"body\", \"subreddit\"])\n","rows_3 = df.count()\n","rows_dropped = rows_2 - rows_3\n","print(f\"Current amount of rows: {rows_3}, NULL rows dropped: {rows_dropped}.\")\n","df.show(10)"]},{"cell_type":"code","execution_count":5,"id":"fa56e70f","metadata":{},"outputs":[],"source":["# Preprocess the data - in accordance with SparkNLP tutorial\n","from pyspark.sql.functions import regexp_replace, length # Function to replace substrings in column using regular expressions and just check length.\n","\n","# Remove empty strings (i.e containing only whitespaces)\n","df = df.filter(~col(\"body\").rlike(\"^\\s*$\"))\n","# Remove Non-ASCII characters like emojis and non-english characters and replaces with spaces.\n","#df = df.withColumn(\"body\", regexp_replace(col(\"body\"), \"[^\\x00-\\x7F]+\", \" \")) \n","# Keep comments with at least 5 letters in one word.\n","#df = df.filter(col(\"body\").rlike(\"[a-zA-Z]{5,}\"))  \n","# Remove punctuation and replace with an empty string to keep words intact\n","#df = df.withColumn(\"body\", regexp_replace(col(\"body\"), \"[!\\\"#$%&'()*+,-./:;<=>?@\\\\[\\\\\\\\\\\\]^_`{|}~]\", \"\"))\n","# Removes very short comments that don't give any meaning really.\n","#df = df.filter(length(col(\"body\")) > 10)\n","#rows_4 = df.count()\n","#print(f\"Rows after pre-processing finalized: {rows_4}, total rows dropped during pre-processing: {rows_2 - rows_4}\")\n","#df.show(10)\n","# After this pre-processing has been finalized"]},{"cell_type":"code","execution_count":6,"id":"4c7e1a33","metadata":{},"outputs":[],"source":["# Rename body column to text\n","df = df.selectExpr(\"body as text\", \"subreddit as subreddit\").repartition(150, \"subreddit\").cache()\n"]},{"cell_type":"code","execution_count":7,"id":"64026a09","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning::Spark Session already created, some configs may not take.\n","sentiment_vivekn download started this may take some time.\n"]},{"name":"stderr","output_type":"stream","text":["25/03/10 20:40:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]},{"name":"stdout","output_type":"stream","text":["Approximate size to download 873.6 KB\n","\r","[ | ]"]},{"name":"stderr","output_type":"stream","text":["25/03/10 20:40:35 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n","25/03/10 20:40:36 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"]},{"name":"stdout","output_type":"stream","text":["sentiment_vivekn download started this may take some time.\n","Approximate size to download 873.6 KB\n","Download done! Loading the resource.\n","[ / ]"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["\r","[ â€” ]"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 15:=======================================================>(79 + 1) / 80]\r","\r","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["[OK!]\n"]}],"source":["import sparknlp\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","from sparknlp.pretrained import PretrainedPipeline\n","\n","from pyspark.ml import Pipeline\n","from pyspark.sql import SparkSession\n","import pyspark.sql.functions as F\n"," \n","spark = sparknlp.start()\n","\n","document = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","token = Tokenizer() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"token\")\n","normalizer = Normalizer() \\\n","    .setInputCols([\"token\"]) \\\n","    .setOutputCol(\"normal\")\n","vivekn = ViveknSentimentModel.pretrained() \\\n","    .setInputCols([\"document\", \"normal\"]) \\\n","    .setOutputCol(\"result_sentiment\")\n","finisher = Finisher() \\\n","    .setInputCols([\"result_sentiment\"]) \\\n","    .setOutputCols(\"final_sentiment\")"]},{"cell_type":"code","execution_count":8,"id":"090cee6a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: An illegal reflective access operation has occurred\n","WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/lib/spark/jars/spark-core_2.12-3.5.1.jar) to field java.util.regex.Pattern.pattern\n","WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n","WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n","WARNING: All illegal access operations will be denied in a future release\n"]}],"source":["pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher])\n","pipelineModel = pipeline.fit(df)\n","result = pipelineModel.transform(df)"]},{"cell_type":"code","execution_count":9,"id":"c9f1f6a1","metadata":{},"outputs":[],"source":["from pyspark.sql import functions as F\n","sentiment_count = result.groupBy(\"subreddit\",\"final_sentiment\").count().filter(col(\"count\") > 50).orderBy(\"subreddit\", \"final_sentiment\")\n","subreddit_counts = sentiment_count.groupBy(\"subreddit\").count()\n","filtered_subreddits = subreddit_counts.filter(F.col(\"count\") > 1).select(\"subreddit\")\n","sentiment_count = sentiment_count.join(filtered_subreddits, on=\"subreddit\", how=\"inner\").orderBy(\"subreddit\", \"final_sentiment\")"]},{"cell_type":"code","execution_count":10,"id":"1d81105e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["sentiment_counts = (\n","    sentiment_count.groupBy(\"subreddit\")\n","    .pivot(\"final_sentiment\")  # Pivot to create separate columns for positive and negative\n","    .sum(\"count\")\n","    .fillna(0)  # Replace nulls with 0 in case a sentiment is missing\n",")"]},{"cell_type":"code","execution_count":11,"id":"70f6ef42","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","== Most positive ==\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+----------------+----+--------------+--------------+------------------+\n","|       subreddit|[na]|negative_count|positive_count|             ratio|\n","+----------------+----+--------------+--------------+------------------+\n","|       wowguilds|   0|           106|           308|2.9056603773584904|\n","|FFXIVRECRUITMENT|   0|            67|           181| 2.701492537313433|\n","|         Romania|   0|           226|           575|2.5442477876106193|\n","| booksuggestions|   0|            76|           190|               2.5|\n","|        dirtyr4r|   0|            73|           152|2.0821917808219177|\n","|         penpals|   0|            63|           131|2.0793650793650795|\n","|guildrecruitment|   0|            60|           114|               1.9|\n","|            EVEX|   0|            95|           179|1.8842105263157896|\n","|       RecruitCS|   0|            51|            93|1.8235294117647058|\n","|     MakeupRehab|   0|            51|            93|1.8235294117647058|\n","+----------------+----+--------------+--------------+------------------+\n","only showing top 10 rows\n","\n","\n","== Most negative ==\n","+-------------------+----+--------------+--------------+-------------------+\n","|          subreddit|[na]|negative_count|positive_count|              ratio|\n","+-------------------+----+--------------+--------------+-------------------+\n","|            Denmark|   0|           338|            87|  0.257396449704142|\n","|             sweden|   0|           831|           265|0.31889290012033694|\n","|  androidcirclejerk|   0|           159|            68| 0.4276729559748428|\n","|      TodayIdreamed|   0|           115|            59| 0.5130434782608696|\n","|  interestingasfuck|   0|           181|            93| 0.5138121546961326|\n","|       empirepowers|   0|           174|            90| 0.5172413793103449|\n","|PhilosophyofScience|   0|           143|            75| 0.5244755244755245|\n","|   MRSelfPostCopies|   0|           133|            70| 0.5263157894736842|\n","|         GamerGhazi|   0|           292|           158|  0.541095890410959|\n","|      tipofmytongue|   0|           471|           256| 0.5435244161358811|\n","+-------------------+----+--------------+--------------+-------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","[Stage 40:==================================================>   (140 + 2) / 150]\r","\r","                                                                                \r"]}],"source":["from pyspark.sql.functions import asc, desc\n","sentiment_counts = sentiment_counts.withColumnRenamed(\"[positive]\", \"positive_count\") \\\n","                                   .withColumnRenamed(\"[negative]\", \"negative_count\")\n","\n","# Compute the positive-to-negative ratio\n","sentiment_ratio = sentiment_counts.withColumn(\n","    \"ratio\", F.col(\"positive_count\") / F.col(\"negative_count\")\n",").cache()\n","\n","print(\"\\n== Most positive ==\")\n","sentiment_ratio.orderBy(desc(\"ratio\")).show(10)\n","\n","\n","print(\"\\n== Most negative ==\")\n","sentiment_ratio.orderBy(asc(\"ratio\")).show(10)"]},{"cell_type":"code","execution_count":47,"id":"c5bb5608","metadata":{},"outputs":[],"source":["spark.stop()"]},{"cell_type":"code","execution_count":null,"id":"1c5fc7f1","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}