{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1287097d-defe-4151-b5cb-fbeb53b50f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/05 18:05:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark setup - alternative setup from Slack as original was not working.\n",
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder \\\n",
    "    .master(\"spark://192.168.2.156:7077\") \\\n",
    "    .appName(\"projectgroup1_reddit-dataset-100k\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True) \\\n",
    "    .config(\"spark.shuffle.service.enabled\", False) \\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\") \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .config(\"spark.driver.port\",9999)\\\n",
    "    .config(\"spark.blockManager.port\",10005)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c638d00c-b36a-4abe-9e6f-5d3a58365075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current amount of rows: 200001\n",
      "+---------------+----------------+--------------------+--------------------+-----------+-------+--------------------+---------+------------+--------------------+-----------+-----+\n",
      "|_corrupt_record|          author|                body|             content|content_len|     id|      normalizedBody|subreddit|subreddit_id|             summary|summary_len|title|\n",
      "+---------------+----------------+--------------------+--------------------+-----------+-------+--------------------+---------+------------+--------------------+-----------+-----+\n",
      "|           NULL|raysofdarkmatter|I think it should...|I think it should...|        178|c69al3r|I think it should...|     math|    t5_2qh0n|Shifting seasonal...|          8| NULL|\n",
      "|              \u0000|            NULL|                NULL|                NULL|       NULL|   NULL|                NULL|     NULL|        NULL|                NULL|       NULL| NULL|\n",
      "+---------------+----------------+--------------------+--------------------+-----------+-------+--------------------+---------+------------+--------------------+-----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Reddit dataset which is up on Spark.\n",
    "file_path = \"hdfs://192.168.2.156:9000/data/reddit/reddit_100k.json\" # For now use a smaller subset to simply test.\n",
    "\n",
    "df = spark_session.read.json(file_path)\n",
    "rows_1 = df.count()\n",
    "print(f\"Current amount of rows: {rows_1}\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49ef65a-f761-439b-84f1-98d1649afb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current amount of rows: 200001\n",
      "+--------------------+-----------+\n",
      "|                body|  subreddit|\n",
      "+--------------------+-----------+\n",
      "|I think it should...|       math|\n",
      "|                NULL|       NULL|\n",
      "|Art is about the ...|      funny|\n",
      "|                NULL|       NULL|\n",
      "|Ask me what I thi...|Borderlands|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Select the only two we wanna analyze, the subreddit name and it's contents.\n",
    "df = df.select(col(\"body\"), col(\"subreddit\"))\n",
    "rows_2 = df.count()\n",
    "print(f\"Current amount of rows: {rows_2}\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4ceeb8-0602-46e8-8b8b-8b5863daa886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current amount of rows: 99668, NULL rows dropped: 100333.\n",
      "+--------------------+--------------------+\n",
      "|                body|           subreddit|\n",
      "+--------------------+--------------------+\n",
      "|I think it should...|                math|\n",
      "|Art is about the ...|               funny|\n",
      "|Ask me what I thi...|         Borderlands|\n",
      "|In Mechwarrior On...|            gamingpc|\n",
      "|You are talking a...|              Diablo|\n",
      "|All but one of my...|   RedditLaqueristas|\n",
      "|I could give a sh...|               apple|\n",
      "|So you're saying ...|               apple|\n",
      "|I love this idea ...|RedditFilmsProduc...|\n",
      "|Theres an entire ...|       AbandonedPorn|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleanup the data by removing null rows as we can see some above.\n",
    "df = df.dropna(subset=[\"body\", \"subreddit\"])\n",
    "rows_3 = df.count()\n",
    "rows_dropped = rows_2 - rows_3\n",
    "print(f\"Current amount of rows: {rows_3}, NULL rows dropped: {rows_dropped}.\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4253124e-d60b-4bfc-b417-0dbf9f4302b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after pre-processing finalized: 99556, total rows dropped during pre-processing: 100445\n",
      "+--------------------+--------------------+\n",
      "|                body|           subreddit|\n",
      "+--------------------+--------------------+\n",
      "|I think it should...|                math|\n",
      "|Art is about the ...|               funny|\n",
      "|Ask me what I thi...|         Borderlands|\n",
      "|In Mechwarrior On...|            gamingpc|\n",
      "|You are talking a...|              Diablo|\n",
      "|All but one of my...|   RedditLaqueristas|\n",
      "|I could give a sh...|               apple|\n",
      "|So youre saying t...|               apple|\n",
      "|I love this idea ...|RedditFilmsProduc...|\n",
      "|Theres an entire ...|       AbandonedPorn|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data - in accordance with SparkNLP tutorial\n",
    "from pyspark.sql.functions import regexp_replace, length # Function to replace substrings in column using regular expressions and just check length.\n",
    "\n",
    "# Remove empty strings (i.e containing only whitespaces)\n",
    "df = df.filter(~col(\"body\").rlike(\"^\\s*$\"))\n",
    "# Remove Non-ASCII characters like emojis and non-english characters and replaces with spaces.\n",
    "df = df.withColumn(\"body\", regexp_replace(col(\"body\"), \"[^\\x00-\\x7F]+\", \" \")) \n",
    "# Keep comments with at least 5 letters in one word.\n",
    "df = df.filter(col(\"body\").rlike(\"[a-zA-Z]{5,}\"))  \n",
    "# Remove punctuation and replace with an empty string to keep words intact\n",
    "df = df.withColumn(\"body\", regexp_replace(col(\"body\"), \"[!\\\"#$%&'()*+,-./:;<=>?@\\\\[\\\\\\\\\\\\]^_`{|}~]\", \"\"))\n",
    "# Removes very short comments that don't give any meaning really.\n",
    "df = df.filter(length(col(\"body\")) > 10)\n",
    "rows_4 = df.count()\n",
    "print(f\"Rows after pre-processing finalized: {rows_4}, total rows dropped during pre-processing: {rows_2 - rows_4}\")\n",
    "df.show(10)\n",
    "# After this pre-processing has been finalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9720f3-4e95-413a-96b9-f61a796f1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop() # Ending our current spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f937c-b24b-4521-9fe4-cefcb57dc506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
